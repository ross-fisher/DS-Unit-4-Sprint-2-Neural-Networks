{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dVfaLrjLvxvQ"
   },
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "# Neural Networks\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2 Assignment 1*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wxtoY12mwmih"
   },
   "source": [
    "## Define the Following:\n",
    "You can add image, diagrams, whatever you need to ensure that you understand the concepts below.\n",
    "\n",
    "### Input Layer:\n",
    "Input nodes such as the ones representing pixels on an image. They do not take info from previous layers.\n",
    "\n",
    "### Hidden Layer:\n",
    "Layers in between the input layer and the output layer to add additional complexity to the model. Tuned \n",
    "by process backpropagation in order to adjust itself and correctly predict the output based on the\n",
    "signals from the input layer.\n",
    "\n",
    "### Output Layer:\n",
    "The target layer in the model. Each node might represent a different classification. Used to asses the model\n",
    "and tune the hidden layers based on whether or not the output layer gave the correct prediction.\n",
    "\n",
    "### Neuron:\n",
    "Node in a graph in relation to neural networks.\n",
    "\n",
    "### Weight:\n",
    "Parameter used to adjust each input in a neuron (which is adjusted by a fitness function and gradient descent). \n",
    "Weights and the inputs are combined and summed to produce a weighted sum.\n",
    "\n",
    "### Activation Function:\n",
    "Might be a step function or a sigmoid function or relu and it maps its input between a range like between 0 and 1.\n",
    "![Activation Functions](https://miro.medium.com/max/1005/1*p_hyqAtyI8pbt2kEl6siOQ.png)\n",
    "\n",
    "### Node Map:\n",
    "A representation of the neural network in terms of a graph with nodes and edges. \n",
    "\n",
    "### Perceptron:\n",
    "Simplest unit of computation in a neural network. It is a binary classifier and is a type of linear classifier.\n",
    "\n",
    "![Perceptron](https://s2.qwant.com/thumbr/0x0/6/b/7237d55675ed90a0458b307f6f7b398237c3c9a2ea9055a23d511b61e81987/Rosenblattperceptron.png?u=https%3A%2F%2Fupload.wikimedia.org%2Fwikipedia%2Fcommons%2Ff%2Fff%2FRosenblattperceptron.png&q=0&b=1&p=0&a=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NXuy9WcWzxa4"
   },
   "source": [
    "## Inputs -> Outputs\n",
    "\n",
    "### Explain the flow of information through a neural network from inputs to outputs. Be sure to include: inputs, weights, bias, and activation functions. How does it all flow from beginning to end?\n",
    "Inputs are mapped into the input layer which send that message forward to neurons in the hidden in the layer.\n",
    "Those neurons take the weights and the inputs from the input layer and inputs into a transfer function, which \n",
    "is probably the weighted sums plus a bias, and that is mapped over the activation function such as sigmoid to \n",
    "produce an output to the next layer. Backpropagation is used based on the output to adjust the weights from\n",
    "earlier steps using gradient descent and the derivative of the activation function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PlSwIJMC0A8F"
   },
   "source": [
    "#### Your Answer Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6sWR43PTwhSk"
   },
   "source": [
    "## Write your own perceptron code that can correctly classify (99.0% accuracy) a NAND gate. \n",
    "\n",
    "| x1 | x2 | y |\n",
    "|----|----|---|\n",
    "| 0  | 0  | 1 |\n",
    "| 1  | 0  | 1 |\n",
    "| 0  | 1  | 1 |\n",
    "| 1  | 1  | 0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "data = { 'x1': [0,1,0,1],\n",
    "         'x2': [0,0,1,1],\n",
    "         'y':  [1,1,1,0]}\n",
    "\n",
    "df = pd.DataFrame.from_dict(data).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sgh7VFGwnXGH"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    sx =  sigmoid(x)\n",
    "    return sx * (1-sx)\n",
    "\n",
    "class Perceptron(object):\n",
    "    def init_weights(self,X):\n",
    "        self.weights = np.random.random((X.shape[1], 1))\n",
    "\n",
    "    def init_bias(self, X):\n",
    "        self.bias = np.random.random((X.shape[0], 1))\n",
    "        \n",
    "    def get_weighted_sum(self, X):\n",
    "        return np.dot(X, self.weights) + self.bias\n",
    "    \n",
    "    def __init__(self,  n=1000):\n",
    "        self.n = n\n",
    "        self.init_weights(X)\n",
    "        self.init_bias(X)\n",
    "    \n",
    "    def feedforward(self, a):\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a) + b)\n",
    "        return a\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        for i in range(self.n):\n",
    "            # the inputs adjusted by weights and bias\n",
    "            weighted_sum = self.get_weighted_sum(X)\n",
    "            # the prediction of the perceptron\n",
    "            activated = sigmoid(weighted_sum) \n",
    "            # adjust based on error gradient descent \n",
    "            err = np.array([y]).T - activated\n",
    "            adjusted = err * sigmoid_derivative(activated)\n",
    "            \n",
    "            # adjust weights\n",
    "            self.weights += np.dot(X.T, adjusted)\n",
    "            self.bias += np.mean(err)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        result = sigmoid(self.get_weighted_sum(X))\n",
    "        return result\n",
    "            \n",
    "    def get_info(self, X):\n",
    "        result = pd.DataFrame(sigmoid( self.get_weighted_sum(X) ).round(), columns=['Predicted'])\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Perceptron()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "_in = ['x1', 'x2']\n",
    "target = 'y'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.fit(df[_in], df[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[11.75304018],\n",
       "       [ 4.04162655],\n",
       "       [ 4.04162676],\n",
       "       [-3.79278704]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.get_weighted_sum(df[_in])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Predicted\n",
       "0        1.0\n",
       "1        1.0\n",
       "2        1.0\n",
       "3        0.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.get_info(df[_in])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'err' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-ea5d7e39dd60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'err' is not defined"
     ]
    }
   ],
   "source": [
    "err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted = err * sigmoid_derivative(activated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update our weights 10,000 times - (fingers crossed that this process reduces error)\n",
    "def train(n=1_000):\n",
    "    inputs = df[_in]\n",
    "    correct_outputs = [df[target]]\n",
    "    weights = init_weights(inputs)\n",
    "    for iteration in range(n):\n",
    "        # Weighted sum of inputs / weights\n",
    "        weighted_sum = np.dot(inputs, weights)\n",
    "        # Activate!\n",
    "        activated_output = sigmoid(weighted_sum)\n",
    "        # Cac error\n",
    "        error = correct_outputs - activated_output\n",
    "        adjustments = error * sigmoid_derivative(activated_output)\n",
    "        # Update the Weights\n",
    "        weights += np.dot(inputs.T, adjustments)\n",
    "    return dict(weights=weights, activated_output=activated_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_info = train()\n",
    "weights_df = pd.DataFrame(train_info['weights'])\n",
    "output_df = pd.DataFrame(train_info['activated_output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xf7sdqVs0s4x"
   },
   "source": [
    "## Implement your own Perceptron Class and use it to classify a binary dataset: \n",
    "- [The Pima Indians Diabetes dataset](https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv) \n",
    "\n",
    "You may need to search for other's implementations in order to get inspiration for your own. There are *lots* of perceptron implementations on the internet with varying levels of sophistication and complexity. Whatever your approach, make sure you understand **every** line of your implementation and what its purpose is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv')\n",
    "diabetes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although neural networks can handle non-normalized data, scaling or normalizing your data will improve your neural network's learning speed. Try to apply the sklearn `MinMaxScaler` or `Normalizer` to your diabetes dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, Normalizer\n",
    "\n",
    "feats = list(diabetes)[:-1]\n",
    "\n",
    "X = diabetes[feats]\n",
    "y = diabetes['Outcome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max = MinMaxScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normed = Normalizer().fit_transform(min_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Perceptron().fit(normed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "pipe = Pipeline(\n",
    "    [\n",
    "        #('min-max', MinMaxScaler()),\n",
    "        ('norm', Normalizer()),\n",
    "        ('percept', Perceptron(n=10_000))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = pipe.named_steps['percept'].predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame({'y' : y, \n",
    " 'predicted' : predicted[:,0].round()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy not great\n",
    "results_df[results_df['y'] != results_df['predicted']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6QR4oAW1xdyu"
   },
   "source": [
    "## Stretch Goals:\n",
    "\n",
    "- Research \"backpropagation\" to learn how weights get updated in neural networks (tomorrow's lecture). \n",
    "- Implement a multi-layer perceptron. (for non-linearly separable classes)\n",
    "- Try and implement your own backpropagation algorithm.\n",
    "- What are the pros and cons of the different activation functions? How should you decide between them for the different layers of a neural network?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_431_Intro_to_NN_Assignment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
